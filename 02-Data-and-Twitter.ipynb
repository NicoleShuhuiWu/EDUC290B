{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Twitter Analysis\n",
    "---\n",
    "\n",
    "This notebook introduces students to common data formats and how Python can read and write them. How one decides to structure data will ultimately shape the storage and possible analyses. After a discussion of the data, an exploration of a Twitter API response will exemplify JSON and tweets from the March 4th, 2017 #March4Trump in Berkeey will serve as a basis for analysis. At the end, students will collect their own data to explore.\n",
    "\n",
    "*Estimated Time: 180 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "**Topics Covered:**\n",
    "- .xls and .csv formats\n",
    "- .html and .xml\n",
    "- .json\n",
    "- Twitter API\n",
    "- Twitter analysis and visualization\n",
    "\n",
    "**Parts:**\n",
    "- [Data Formats and Storage](#dataformats)\n",
    "- [APIs](#APIs)\n",
    "- [Twitter API](#twitterapi)\n",
    "- [March4Trump](#march)\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! pip install tweepy\n",
    "! pip install textblob\n",
    "! pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=dataformats></a>\n",
    "## Data Formats and Storage\n",
    "\n",
    "### CSV\n",
    "\n",
    "Most people are familiar with Microsoft Excel spreadsheet's `.xls` format, great for storing tabular data. However, Microsoft encodes the `.xls` format with a lot of information for displaying it in the software environment as well as remembering any formulas you may have used, among other things. The extra information is often not necessary to simply store the raw data, and is not easily readable by other software. A \"bare-bones\" `.xls` format is the `.csv`, or \"comma-separated value\". You may have encountered this format before. It's not any more complicated than the name. All values are separated by commas to delimit columns, while the lines represent rows.\n",
    "\n",
    "The table:\n",
    "\n",
    "| Name    | Age | Department | Hometown |\n",
    "|---------|-----|------------|----------|\n",
    "| Chris   | 27  | German     | Plymouth |\n",
    "| Jarrett | 25  | Physics    | Newark   |\n",
    "| Sofia   | 22  | Chemistry  | Boston   |\n",
    "| Esther  | 24  | Economics  | Oakland  |\n",
    "\n",
    "\n",
    "would be represented as:\n",
    "\n",
    "~~~\n",
    "Name, Age, Department, Hometown\n",
    "Chris, 27, German, Plymouth\n",
    "Jarrett, 25, Physics, Newark\n",
    "Sofia, 22, Chemistry, Boston\n",
    "Esther, 24, Economics, Oakland\n",
    "~~~\n",
    "\n",
    "Notably, the header is not distinguishable except for being the first row. There is also no way to add any metadata or notes unless it fits into a column or row. Nevertheless, `.csv` is standard for simple data, and is easily read by most software. If you are collaborating with researchers or using different pieces of software you'll want to use this format.\n",
    "\n",
    "Python can easily dump data into a `.csv`, the most straight-forward approach would be dumping rows from a list of lists, each sublist being a row in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "my_data = [['Name', 'Age', 'Department', 'Hometown'],\n",
    "            ['Chris', '27', 'German', 'Plymouth'],\n",
    "            ['Jarrett', '25', 'Physics', 'Newark',],\n",
    "            ['Sofia', '22', 'Chemistry', 'Boston'],\n",
    "            ['Esther', '24', 'Economics', 'Oakland']\n",
    "        ]\n",
    "\n",
    "with open(\"my_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a `.csv` is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"my_data.csv\", \"r\") as f:\n",
    "    csv_data = list(csv.reader(f))\n",
    "    \n",
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still prefer Excel for analysis, you can go ahead and open this file in Excel!\n",
    "\n",
    "---\n",
    "\n",
    "### XML\n",
    "\n",
    "XML, or 'Extensible Markup Language', much like HTML is structured by tags. Each tag will have a beginning tag and an end tag. The end tag is marked with a `/` before the tag name. Unlike HTML, XML does not have pre-defined tags that have certain functions, so we have to come up with our own. XML is a great way to structure metadata, and is commonly used for onilne data and annotating corpora. Let's look at an example:\n",
    "\n",
    "~~~\n",
    "\n",
    "<my-library>\n",
    "    <book>\n",
    "        <title>Harry Potter and the Sorcerer's Stone</title>\n",
    "        <author>J. K. Rowling</author>\n",
    "        <date>1998</date>\n",
    "        <publisher>Scholastic Corporation</publisher>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>The Hobbit</title>\n",
    "        <author>J. R. R. Tolkien</author>\n",
    "        <date>1937</date>\n",
    "        <publisher>George Allen and Unwin</publisher>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>To Kill A Mockingbird</title>\n",
    "        <author>Harper Lee</author>\n",
    "        <date>1960</date>\n",
    "        <publisher>J. B. Lippincott and Co.</publisher>\n",
    "    </book>\n",
    "</my-library>\n",
    "\n",
    "~~~\n",
    "\n",
    "You could, of course, use a CSV for this data, but when there are several more categories (if I wanted to add `films`, for example) it can get messy very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_data = '''\n",
    "<my-library>\n",
    "    <book>\n",
    "        <title>Harry Potter and the Sorcerer's Stone</title>\n",
    "        <author>J. K. Rowling</author>\n",
    "        <date>1998</date>\n",
    "        <publisher>Scholastic Corporation</publisher>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>The Hobbit</title>\n",
    "        <author>J. R. R. Tolkien</author>\n",
    "        <date>1937</date>\n",
    "        <publisher>George Allen and Unwin</publisher>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>To Kill A Mockingbird</title>\n",
    "        <author>Harper Lee</author>\n",
    "        <date>1960</date>\n",
    "        <publisher>J. B. Lippincott and Co.</publisher>\n",
    "    </book>\n",
    "</my-library>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "\n",
    "e = xml.etree.ElementTree.fromstring(xml_data)\n",
    "e.findall('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(e.findall('book')[0][0].text)\n",
    "print(e.findall('book')[0][1].text)\n",
    "print(e.findall('book')[0][2].text)\n",
    "print(e.findall('book')[0][3].text)\n",
    "print()\n",
    "print(e.findall('book')[1][0].text)\n",
    "print(e.findall('book')[1][1].text)\n",
    "print(e.findall('book')[1][2].text)\n",
    "print(e.findall('book')[1][3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JSON (JavaScript Object Notation) is a format for structuring and exchanging data. Its syntax is based on JavaScript, but you can still use it in any language, including Python. Its format is somewhat similar to that of a Python dictionary in that it consists of a collection of key-value pairs. JSON, along with XML, are the most popular formats to get data from the internet. Let's look at the same data from the XML example in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = {'my-library': [{'title': \"Harry Potter and the Sorcerer's Stone\",\n",
    "                            'author': 'J. K. Rowling',\n",
    "                            'date': '1998',\n",
    "                            'publisher': 'Scholastic Corporation'},\n",
    "                            \n",
    "                            {'title': \"The Hobbit\",\n",
    "                            'author': 'J. R. R. Tolkien',\n",
    "                            'date': '1937',\n",
    "                            'publisher': 'George Allen and Unwin'},\n",
    "                            \n",
    "                            {'title': \"To Kill A Mockingbird\",\n",
    "                            'author': 'Harper Lee',\n",
    "                            'date': '1960',\n",
    "                            'publisher': 'J. B. Lippincott and Co.'},\n",
    "                            ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(json_data['my-library'][0])\n",
    "print()\n",
    "print(json_data['my-library'][1])\n",
    "print()\n",
    "print(json_data['my-library'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=APIs></a>\n",
    "## APIs\n",
    "\n",
    "### What is an API?\n",
    "\n",
    "\n",
    "* API stands for **Application Programming Interface**\n",
    "\n",
    "* a set of rules and procedures that facilitate interactions between computers and their applications\n",
    "\n",
    "---\n",
    "### Web APIs\n",
    "\n",
    "* allows users to query a remote database over the internet\n",
    "\n",
    "* take on a variety of formats \n",
    "\n",
    "* majority adhere to a particular style known as **Reperesentational State Transfer** or **REST**\n",
    "\n",
    "* \"RESTful\" APIs are conveinent because we can use them to query databases using URLs \n",
    "\n",
    "\n",
    "---\n",
    "### RESTful Web APIs are All Around You...\n",
    "\n",
    "Consider a simple Google search.\n",
    "\n",
    "Go ahead and search something.\n",
    "\n",
    "Ever wonder what all that extra stuff in the address bar was all about?  \n",
    "\n",
    "---\n",
    "### RESTful Web APIs are All Around You...\n",
    "\n",
    "It looks like Google makes its query by taking the search terms, separating each of them with a \"`+`\", and appending them to the link:\n",
    "\n",
    "`https://www.google.com/#q=`\n",
    "\n",
    "So that we have\n",
    "\n",
    "`https://www.google.com/#q=search1+search2`\n",
    "\n",
    "So can change our Google search by adding some terms to the URL.\n",
    "\n",
    "---\n",
    "### Some Basic Terminology: URL\n",
    "\n",
    "* Uniform Resource Location\n",
    "* a string of characters that, when interpreted via the Hypertext Transfer Protocol (HTTP)\n",
    "* points to a data resource, notably files written in Hypertext Markup Language (HTML) or a subset of a database.\n",
    "\n",
    "---\n",
    "### Some Basic Terminology: HTTP Methods / Verbs\n",
    "\n",
    "* *GET*: requests a representation of a data resource corresponding to a particular URL.  The process of executing the GET method is often referred to as a \"GET request\" and is the main method used for querying RESTful databases.\n",
    "    \n",
    "*  *HEAD*, *POST*, *PUT*, *DELETE*: other common methods, though mostly never used for database querying.\n",
    "\n",
    "---\n",
    "### How Do GET Requests Work?  A Web Browsing Example\n",
    "\n",
    "* Surfing the Web = Making a bunch of GET Requests\n",
    "\n",
    "* For instance, I open my web browser and type in http://www.wikipedia.org.  Once I hit return, I'd see a webpage.\n",
    "\n",
    "* Several different processes occured, however, between me hitting \"return\" and the page finally being rendered. \n",
    "\n",
    "---\n",
    "### Step 1: The GET Request\n",
    "\n",
    "* web browser took the entered character string \n",
    "* used the command-line tool \"Curl\" to write a properly formatted HTTP GET request \n",
    "* submitted it to the server that hosts the Wikipedia homepage.\n",
    "\n",
    "---\n",
    "### STEP 2: The Response\n",
    "\n",
    "* Wikipedia's server receives this request\n",
    "* send back an HTTP response\n",
    "* from which Curl extracted the HTML code for the page\n",
    "\n",
    "```{html}\n",
    "[1] \"<!DOCTYPE html>\\n<html lang=\\\"mul\\\" dir=\\\"ltr\\\">\\n<head>\\n<!-- Sysops: Please do not edit the main template directly; update /temp and synchronise. -->\\n<meta charset=\\\"utf-8\\\">\\n<title>Wikipedia</title>\\n<!--[if lt IE 7]><meta http-equiv=\\\"imagetoolbar\\\" content=\\\"no\\\"><![endif]-->\\n<meta name=\\\"viewport\\\" content=\\\"i\"\n",
    "```\n",
    "\n",
    "---\n",
    "### STEP 3: The Formatting\n",
    "\n",
    "* raw HTML code was formatted and executed by the web browser\n",
    "* rendering the page as seen in the window.\n",
    "\n",
    "---\n",
    "### RESTful Database Querying: The GET Request\n",
    "\n",
    "* URL we supply must be constructed so that the resulting request can be interpreted and succesfully acted upon by the server.  \n",
    "\n",
    "* Likely that the character string must encode **search terms and/or filtering parameters**, as well as one or more **authentication codes**.  \n",
    "\n",
    "* While the terms are often similar across APIs, most are API-specific.\n",
    "\n",
    "---\n",
    "### RESTful Database Querying: The Response\n",
    "\n",
    "* unlike web browsing, the content of the server's response that is extracted by Curl is unlikely to be HTML code. \n",
    "\n",
    "* will likely be **raw text** response that can be parsed into one of a few file formats commonly used for data storage.  \n",
    "\n",
    "* usual suspects include .csv, .xml, and .json files.\n",
    "\n",
    "---\n",
    "### RESTful Database Querying: The Formatting\n",
    "\n",
    "* web browser parsed the HTML code, \n",
    "* but **we need R, Python, or other programming languages** to parse the server response \n",
    "* and convert it into a format for local storage (e.g. matrices, dataframes, databases, lists, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "Let's look at an example of restaurant health inspections in Chicago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['meta']['view']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's structured just like our book library data above, but defintely has a lot more information!\n",
    "\n",
    "---\n",
    "<a id=twitterapi></a>\n",
    "## Twitter API\n",
    "\n",
    "This Twitter API is slightly more complicated, but because of this, people have created very useful tools to easily interact with the Twitter API. First, follow the directions in the `Install.md` file of this repository to get your API credientials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "\n",
    "# Twitter API credentials\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search all twitter\n",
    "results = tweepy.Cursor(\n",
    "    api.search,\n",
    "    q='Berkeley', # query, any word you want found in a tweet\n",
    "    geocode=\"37.871853,-122.258423,80km\", # lat. and long., radius\n",
    "    ).items(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get tweets from specific handle\n",
    "handle_results = api.user_timeline(screen_name='UCBerkeley', count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will return an iterator, which means it has not actually downloaded the data yet, so we must iterate through to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_tweets = []\n",
    "\n",
    "for t in results:\n",
    "    results_tweets.append(t)\n",
    "    \n",
    "handle_results_tweets = []\n",
    "\n",
    "for t in handle_results:\n",
    "    handle_results_tweets.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(results_tweets))\n",
    "print()\n",
    "print(results_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Look at all that data! Let's look at one tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(results_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be useful here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(results_tweets[0].created_at)\n",
    "print()\n",
    "print(results_tweets[0].text)\n",
    "print()\n",
    "print(results_tweets[0].retweet_count)\n",
    "print()\n",
    "print(results_tweets[0].entities['user_mentions'])\n",
    "print()\n",
    "print(results_tweets[0].entities['hashtags'])\n",
    "print()\n",
    "print(results_tweets[0].entities['urls'])\n",
    "print()\n",
    "print(results_tweets[0].user.profile_image_url_https)\n",
    "print()\n",
    "print(results_tweets[0].geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at our tweets from the Berkeley handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(handle_results_tweets))\n",
    "print()\n",
    "print(handle_results_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in results_tweets:\n",
    "    print(t.created_at.strftime(\"%Y-%b-%d %H:%M\"))\n",
    "    print(t.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in handle_results_tweets:\n",
    "    print(t.created_at.strftime(\"%Y-%b-%d %H:%M\"))\n",
    "    print(t.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tweet_words = []\n",
    "\n",
    "for t in results_tweets:\n",
    "    tweet_words.extend(t.text.split())\n",
    "\n",
    "Counter(tweet_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "\n",
    "for t in results_tweets:\n",
    "    for h in t.entities[\"hashtags\"]:\n",
    "        hashtags.append(h['text'])\n",
    "        \n",
    "Counter(hashtags).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions= []\n",
    "\n",
    "for t in results_tweets:\n",
    "    for m in t.entities[\"user_mentions\"]:\n",
    "        mentions.append(m[\"screen_name\"])\n",
    "        \n",
    "Counter(mentions).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for t in results_tweets:\n",
    "\n",
    "    blob = TextBlob(t.text)\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        pol = sentence.sentiment.polarity\n",
    "        print(pol, sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=march></a>\n",
    "## March4Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = pickle.load(open(\"tweets.pkl\", \"rb\"))\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "hashtags = []\n",
    "user_mentions = []\n",
    "texts = []\n",
    "\n",
    "pattern = re.compile(r'http\\S+')\n",
    "pattern2 = re.compile(r'(#|@)[^:\\s]+')\n",
    "\n",
    "for t in data:\n",
    "    for h in t.entities[\"hashtags\"]:\n",
    "        hashtags.append(h[\"text\"])\n",
    "\n",
    "    for m in t.entities[\"user_mentions\"]:\n",
    "        user_mentions.append(m[\"screen_name\"])\n",
    "\n",
    "\n",
    "    original_tweet = t.text\n",
    "    text = re.sub(pattern, \"\", t.text)\n",
    "    text = re.sub(pattern2, \"\", text)\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"RT\"):\n",
    "        text = ' '.join(text.split(\":\")[1:]).strip()\n",
    "\n",
    "    texts.append(text)\n",
    "        \n",
    "print(Counter(hashtags).most_common())\n",
    "print()\n",
    "print(\"=\"*20)\n",
    "print()\n",
    "print(Counter(user_mentions).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "user_mentions = []\n",
    "texts = []\n",
    "\n",
    "pattern = re.compile(r'http\\S+')\n",
    "pattern2 = re.compile(r'(#|@)[^:\\s]+')\n",
    "\n",
    "for t in data:\n",
    "    if not t.text.startswith(\"RT\"):\n",
    "        for h in t.entities[\"hashtags\"]:\n",
    "            hashtags.append(h[\"text\"])\n",
    "\n",
    "        for m in t.entities[\"user_mentions\"]:\n",
    "            user_mentions.append(m[\"screen_name\"])\n",
    "\n",
    "\n",
    "        original_tweet = t.text\n",
    "        text = re.sub(pattern, \"\", t.text)\n",
    "        text = re.sub(pattern2, \"\", text)\n",
    "        text = text.strip()\n",
    "        if text.startswith(\"RT\"):\n",
    "            text = ' '.join(text.split(\":\")[1:]).strip()\n",
    "\n",
    "        texts.append(text)\n",
    "        \n",
    "print(Counter(hashtags).most_common())\n",
    "print()\n",
    "print(\"=\"*10)\n",
    "print()\n",
    "print(Counter(user_mentions).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lats = []\n",
    "longs = []\n",
    "tweet_texts = []\n",
    "\n",
    "for t in data:\n",
    "    if t.geo:\n",
    "        print(t.geo)\n",
    "        lats.append(t.geo['coordinates'][0])\n",
    "        longs.append(t.geo['coordinates'][1])\n",
    "        tweet_texts.append(t.text.replace('\\n', '<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the [Mapbox](https://www.mapbox.com) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "mapbox_access_token = ''\n",
    "\n",
    "map_data = Data([\n",
    "    Scattermapbox(\n",
    "        lat=lats,\n",
    "        lon=longs,\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            size=14\n",
    "        ),\n",
    "        text=tweet_texts,\n",
    "    )\n",
    "])\n",
    "\n",
    "layout = Layout(\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(\n",
    "            lat=37.85,\n",
    "            lon=-122.28\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=7\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=map_data, layout=layout)\n",
    "py.iplot(fig, filename='Berkeley Tweet Map', validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_rows = []\n",
    "\n",
    "for t in data:\n",
    "    tweet_data = {}\n",
    "    tweet_data['created'] = t.created_at\n",
    "    tweet_data['user'] = t.user.screen_name\n",
    "    tweet_data['text'] = t.text\n",
    "    tweet_data['retweet_count'] = t.retweet_count\n",
    "    tweet_data['hashtags'] = '; '.join([d['text'] for d in t.entities['hashtags']])\n",
    "    tweet_data['user_mentions'] = '; '.join([d['screen_name'] for d in t.entities['user_mentions']])\n",
    "    tweet_data['urls'] = '; '.join([d['url'] for d in t.entities['urls']])\n",
    "    \n",
    "    if t.geo:\n",
    "        tweet_data['coordinates'] = t.geo['coordinates']\n",
    "    else:\n",
    "        tweet_data['coordinates'] = 'NA'\n",
    "    \n",
    "    tweet_rows.append(tweet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "keys = tweet_rows[0].keys()\n",
    "\n",
    "with open('tweet-data.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(tweet_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn! Get tweets by either searching text or from a specific user and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search all twitter\n",
    "my_tweets_result = tweepy.Cursor(\n",
    "    api.search,\n",
    "    q='Berkeley', # query, any word you want found in a tweet\n",
    "    geocode=\"37.871853,-122.258423,80km\", # lat. and long., radius\n",
    "    ).items(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get tweets from specific handle\n",
    "my_tweets_result = api.user_timeline(screen_name='UCBerkeley', count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tweets = []\n",
    "\n",
    "for t in my_tweets_result:\n",
    "    my_tweets.append(t)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
